{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db033198",
   "metadata": {},
   "source": [
    "# 1. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f85728",
   "metadata": {},
   "source": [
    "## 1.1. Preprocessing\n",
    "The first NLP exercise is about preprocessing.</p></p>\n",
    "You will practice preprocessing using NLTK on raw data.</p>\n",
    "This is the first step in most of the NLP projects, so you have to master it.</p>\n",
    "Open the Preprocessing.ipynb notebook and follow the instructions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9847da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "nltk.download('punkt') #Run this line one time to get the resource\n",
    "nltk.download('stopwords') #Run this line one time to get the resource\n",
    "nltk.download('wordnet') #Run this line one time to get the resource\n",
    "nltk.download('averaged_perceptron_tagger') #Run this line one time to get the resource\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b27d8",
   "metadata": {},
   "source": [
    "Load now the dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9795877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Link</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Another's Arms</td>\n",
       "      <td>/c/coldplay/anothers+arms_21079526.html</td>\n",
       "      <td>Late night watching tv  \\nUsed to be you here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Bigger Stronger</td>\n",
       "      <td>/c/coldplay/bigger+stronger_20032648.html</td>\n",
       "      <td>I want to be bigger stronger drive a faster ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>/c/coldplay/daylight_20032625.html</td>\n",
       "      <td>To my surprise, and my delight  \\nI saw sunris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Everglow</td>\n",
       "      <td>/c/coldplay/everglow_21104546.html</td>\n",
       "      <td>Oh, they say people come  \\nThey say people go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Every Teardrop Is A Waterfall</td>\n",
       "      <td>/c/coldplay/every+teardrop+is+a+waterfall_2091...</td>\n",
       "      <td>I turn the music up, I got my records on  \\nI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Artist                           Song  \\\n",
       "0  Coldplay                 Another's Arms   \n",
       "1  Coldplay                Bigger Stronger   \n",
       "2  Coldplay                       Daylight   \n",
       "3  Coldplay                       Everglow   \n",
       "4  Coldplay  Every Teardrop Is A Waterfall   \n",
       "\n",
       "                                                Link  \\\n",
       "0            /c/coldplay/anothers+arms_21079526.html   \n",
       "1          /c/coldplay/bigger+stronger_20032648.html   \n",
       "2                 /c/coldplay/daylight_20032625.html   \n",
       "3                 /c/coldplay/everglow_21104546.html   \n",
       "4  /c/coldplay/every+teardrop+is+a+waterfall_2091...   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Late night watching tv  \\nUsed to be you here ...  \n",
       "1  I want to be bigger stronger drive a faster ca...  \n",
       "2  To my surprise, and my delight  \\nI saw sunris...  \n",
       "3  Oh, they say people come  \\nThey say people go...  \n",
       "4  I turn the music up, I got my records on  \\nI ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset in coldplay.csv\n",
    "coldplay = pd.read_csv('coldplay.csv')\n",
    "coldplay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d7754",
   "metadata": {},
   "source": [
    "Now, check the dataset, play with it a bit: what are the columns? How many lines? Is there missing data?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8013865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore the data\n",
    "coldplay.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a03862",
   "metadata": {},
   "source": [
    "Now select the song 'Every Teardrop Is A Waterfall' and save the Lyrics text into a variable. Print the output of this variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f744ecc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    I turn the music up, I got my records on  \\nI ...\n",
       "Name: Lyrics, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Select the song 'Every Teardrop Is A Waterfall'\n",
    "song = coldplay.loc[coldplay[\"Song\"] == 'Every Teardrop Is A Waterfall', 'Lyrics']\n",
    "song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e5076",
   "metadata": {},
   "source": [
    "As you can see, there is some preprocessing needed here. So let's do it! What is usually the first step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7acb7",
   "metadata": {},
   "source": [
    "Tokenization, yes. So do tokenization on the lyrics of Every Teardrop Is A Waterfall.\n",
    "\n",
    "So you may have to import the needed library from NLTK if you did not yet.\n",
    "\n",
    "Be careful, the output you have from your pandas dataframe may not have the right type, so manipulate it wisely to get a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424c0b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " ',',\n",
       " 'I',\n",
       " 'got',\n",
       " 'my',\n",
       " 'records',\n",
       " 'on',\n",
       " 'I',\n",
       " 'shut',\n",
       " 'the',\n",
       " 'world',\n",
       " 'outside',\n",
       " 'until',\n",
       " 'the',\n",
       " 'lights',\n",
       " 'come',\n",
       " 'on',\n",
       " 'Maybe',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'alight',\n",
       " ',',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'trees',\n",
       " 'are',\n",
       " 'gone',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'start',\n",
       " 'beating',\n",
       " 'to',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'song',\n",
       " 'And',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'they',\n",
       " 'dance',\n",
       " ',',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'all',\n",
       " 'night',\n",
       " 'Until',\n",
       " 'Monday',\n",
       " 'morning',\n",
       " 'feels',\n",
       " 'another',\n",
       " 'life',\n",
       " 'I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'on',\n",
       " 'a',\n",
       " 'roll',\n",
       " 'this',\n",
       " 'time',\n",
       " 'And',\n",
       " 'heaven',\n",
       " 'is',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " ',',\n",
       " 'I',\n",
       " 'got',\n",
       " 'my',\n",
       " 'records',\n",
       " 'on',\n",
       " 'From',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'rubble',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'rebel',\n",
       " 'song',\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'another',\n",
       " 'generation',\n",
       " 'drop',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'rather',\n",
       " 'be',\n",
       " 'a',\n",
       " 'comma',\n",
       " 'than',\n",
       " 'a',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'black',\n",
       " ',',\n",
       " 'maybe',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'on',\n",
       " 'my',\n",
       " 'knees',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'trapezes',\n",
       " 'But',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'is',\n",
       " 'beating',\n",
       " 'and',\n",
       " 'my',\n",
       " 'pulses',\n",
       " 'start',\n",
       " 'Cathedrals',\n",
       " 'in',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'As',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'oh',\n",
       " 'this',\n",
       " 'light',\n",
       " 'I',\n",
       " 'swear',\n",
       " 'you',\n",
       " ',',\n",
       " 'emerge',\n",
       " 'blinking',\n",
       " 'into',\n",
       " 'To',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'alright',\n",
       " 'As',\n",
       " 'we',\n",
       " 'soar',\n",
       " 'walls',\n",
       " ',',\n",
       " 'every',\n",
       " 'siren',\n",
       " 'is',\n",
       " 'a',\n",
       " 'symphony',\n",
       " 'And',\n",
       " 'every',\n",
       " 'tear',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'So',\n",
       " 'you',\n",
       " 'can',\n",
       " 'hurt',\n",
       " ',',\n",
       " 'hurt',\n",
       " 'me',\n",
       " 'bad',\n",
       " 'But',\n",
       " 'still',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'raise',\n",
       " 'the',\n",
       " 'flag',\n",
       " 'Oh',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa-aterfall',\n",
       " 'A',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa-aterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Tokenize the lyrics of the song and save the tokens into a variable and print it\n",
    "from nltk.tokenize import word_tokenize\n",
    "lyrics = song.apply(word_tokenize)\n",
    "for i in lyrics:\n",
    "    lyrics = i\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e3129",
   "metadata": {},
   "source": [
    "It begins to look good. But still, we have the punctuation to remove, so let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f056c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'I',\n",
       " 'got',\n",
       " 'my',\n",
       " 'records',\n",
       " 'on',\n",
       " 'I',\n",
       " 'shut',\n",
       " 'the',\n",
       " 'world',\n",
       " 'outside',\n",
       " 'until',\n",
       " 'the',\n",
       " 'lights',\n",
       " 'come',\n",
       " 'on',\n",
       " 'Maybe',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'alight',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'trees',\n",
       " 'are',\n",
       " 'gone',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'start',\n",
       " 'beating',\n",
       " 'to',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'song',\n",
       " 'And',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'they',\n",
       " 'dance',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'all',\n",
       " 'night',\n",
       " 'Until',\n",
       " 'Monday',\n",
       " 'morning',\n",
       " 'feels',\n",
       " 'another',\n",
       " 'life',\n",
       " 'I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'I',\n",
       " 'on',\n",
       " 'a',\n",
       " 'roll',\n",
       " 'this',\n",
       " 'time',\n",
       " 'And',\n",
       " 'heaven',\n",
       " 'is',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'I',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'I',\n",
       " 'got',\n",
       " 'my',\n",
       " 'records',\n",
       " 'on',\n",
       " 'From',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'rubble',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'rebel',\n",
       " 'song',\n",
       " 'Do',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'another',\n",
       " 'generation',\n",
       " 'drop',\n",
       " 'I',\n",
       " 'rather',\n",
       " 'be',\n",
       " 'a',\n",
       " 'comma',\n",
       " 'than',\n",
       " 'a',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " 'in',\n",
       " 'the',\n",
       " 'black',\n",
       " 'maybe',\n",
       " 'I',\n",
       " 'on',\n",
       " 'my',\n",
       " 'knees',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " 'in',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'trapezes',\n",
       " 'But',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'is',\n",
       " 'beating',\n",
       " 'and',\n",
       " 'my',\n",
       " 'pulses',\n",
       " 'start',\n",
       " 'Cathedrals',\n",
       " 'in',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'As',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'oh',\n",
       " 'this',\n",
       " 'light',\n",
       " 'I',\n",
       " 'swear',\n",
       " 'you',\n",
       " 'emerge',\n",
       " 'blinking',\n",
       " 'into',\n",
       " 'To',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'it',\n",
       " 'alright',\n",
       " 'As',\n",
       " 'we',\n",
       " 'soar',\n",
       " 'walls',\n",
       " 'every',\n",
       " 'siren',\n",
       " 'is',\n",
       " 'a',\n",
       " 'symphony',\n",
       " 'And',\n",
       " 'every',\n",
       " 'tear',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'So',\n",
       " 'you',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'hurt',\n",
       " 'me',\n",
       " 'bad',\n",
       " 'But',\n",
       " 'still',\n",
       " 'I',\n",
       " 'raise',\n",
       " 'the',\n",
       " 'flag',\n",
       " 'Oh',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'A',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'tear',\n",
       " 'Every',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfall']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Remove the punctuation, then save the result into a variable and print it\n",
    "lyrics = [t for t in lyrics if t.isalpha()]\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096b1f2",
   "metadata": {},
   "source": [
    "We will now remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5cf9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'music', 'I', 'got', 'records', 'I', 'shut', 'world', 'outside', 'lights', 'come', 'Maybe', 'streets', 'alight', 'maybe', 'trees', 'gone', 'I', 'feel', 'heart', 'start', 'beating', 'favourite', 'song', 'And', 'kids', 'dance', 'kids', 'night', 'Until', 'Monday', 'morning', 'feels', 'another', 'life', 'I', 'turn', 'music', 'I', 'roll', 'time', 'And', 'heaven', 'sight', 'I', 'turn', 'music', 'I', 'got', 'records', 'From', 'underneath', 'rubble', 'sing', 'rebel', 'song', 'Do', 'want', 'see', 'another', 'generation', 'drop', 'I', 'rather', 'comma', 'full', 'stop', 'Maybe', 'I', 'black', 'maybe', 'I', 'knees', 'Maybe', 'I', 'gap', 'two', 'trapezes', 'But', 'heart', 'beating', 'pulses', 'start', 'Cathedrals', 'heart', 'As', 'saw', 'oh', 'light', 'I', 'swear', 'emerge', 'blinking', 'To', 'tell', 'alright', 'As', 'soar', 'walls', 'every', 'siren', 'symphony', 'And', 'every', 'tear', 'waterfall', 'Is', 'waterfall', 'Oh', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'waterfall', 'Every', 'tear', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'hurt', 'hurt', 'bad', 'But', 'still', 'I', 'raise', 'flag', 'Oh', 'It', 'wa', 'wa', 'wa', 'wa', 'A', 'wa', 'wa', 'wa', 'wa', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "# TODO: remove the stop words using NLTK. Then put the result into a variable and print it\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lyrics_no_stops = [t for t in lyrics if t not in stop_words]\n",
    "print(lyrics_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5d34c",
   "metadata": {},
   "source": [
    "Okay we begin to have much less words in our song, right?\n",
    "\n",
    "Next step is lemmatization. But we had an issue in the lectures, you remember? Let's learn how to do it properly now.\n",
    "\n",
    "First let's try to do it naively. Import the WordNetLemmatizer and perform lemmatization with default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9879b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform lemmatization using WordNetLemmatizer on our tokens\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Let's import the libraries for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Then we have to create an instance\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a809897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'music', 'I', 'got', 'record', 'I', 'shut', 'world', 'outside', 'light', 'come', 'Maybe', 'street', 'alight', 'maybe', 'tree', 'gone', 'I', 'feel', 'heart', 'start', 'beating', 'favourite', 'song', 'And', 'kid', 'dance', 'kid', 'night', 'Until', 'Monday', 'morning', 'feel', 'another', 'life', 'I', 'turn', 'music', 'I', 'roll', 'time', 'And', 'heaven', 'sight', 'I', 'turn', 'music', 'I', 'got', 'record', 'From', 'underneath', 'rubble', 'sing', 'rebel', 'song', 'Do', 'want', 'see', 'another', 'generation', 'drop', 'I', 'rather', 'comma', 'full', 'stop', 'Maybe', 'I', 'black', 'maybe', 'I', 'knee', 'Maybe', 'I', 'gap', 'two', 'trapeze', 'But', 'heart', 'beating', 'pulse', 'start', 'Cathedrals', 'heart', 'As', 'saw', 'oh', 'light', 'I', 'swear', 'emerge', 'blinking', 'To', 'tell', 'alright', 'As', 'soar', 'wall', 'every', 'siren', 'symphony', 'And', 'every', 'tear', 'waterfall', 'Is', 'waterfall', 'Oh', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'waterfall', 'Every', 'tear', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'hurt', 'hurt', 'bad', 'But', 'still', 'I', 'raise', 'flag', 'Oh', 'It', 'wa', 'wa', 'wa', 'wa', 'A', 'wa', 'wa', 'wa', 'wa', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform lemmatization using WordNetLemmatizer on our tokens\n",
    "lem_words = [lemmatizer.lemmatize(w) for w in lyrics_no_stops]\n",
    "print(lem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aea3cf",
   "metadata": {},
   "source": [
    "As you can see, it worked well on nouns (plural words are now singular for example).\n",
    "\n",
    "But verbs are not OK: we would 'is' to become 'be' for example.\n",
    "\n",
    "To do that, we need to do POS-tagging. So let's do this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e9faf",
   "metadata": {},
   "source": [
    "POS-tagging means Part of speech tagging: basically it will classify words into categories: like verbs, nouns, advers and so on...\n",
    "\n",
    "In order to do that, we will use NLTK and the function *pos_tag*. You have to do it on the step before lemmatization, so use your variable containing all the tokens without punctuation and without stop words.\n",
    "\n",
    "Hint: you can check on the internet how the *pos_tag* function works [here](https://www.nltk.org/book/ch05.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad239ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('turn', 'VBP'),\n",
       " ('music', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('got', 'VBD'),\n",
       " ('record', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('shut', 'VBP'),\n",
       " ('world', 'NN'),\n",
       " ('outside', 'IN'),\n",
       " ('light', 'JJ'),\n",
       " ('come', 'VBP'),\n",
       " ('Maybe', 'NNP'),\n",
       " ('street', 'NN'),\n",
       " ('alight', 'VBD'),\n",
       " ('maybe', 'RB'),\n",
       " ('tree', 'JJ'),\n",
       " ('gone', 'VBN'),\n",
       " ('I', 'PRP'),\n",
       " ('feel', 'VBP'),\n",
       " ('heart', 'NN'),\n",
       " ('start', 'NN'),\n",
       " ('beating', 'VBG'),\n",
       " ('favourite', 'NN'),\n",
       " ('song', 'NN'),\n",
       " ('And', 'CC'),\n",
       " ('kid', 'NN'),\n",
       " ('dance', 'NN'),\n",
       " ('kid', 'NN'),\n",
       " ('night', 'NN'),\n",
       " ('Until', 'IN'),\n",
       " ('Monday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " ('feel', 'NN'),\n",
       " ('another', 'DT'),\n",
       " ('life', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('turn', 'VBP'),\n",
       " ('music', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('roll', 'VBP'),\n",
       " ('time', 'NN'),\n",
       " ('And', 'CC'),\n",
       " ('heaven', 'JJ'),\n",
       " ('sight', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('turn', 'VBP'),\n",
       " ('music', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('got', 'VBD'),\n",
       " ('record', 'NN'),\n",
       " ('From', 'IN'),\n",
       " ('underneath', 'JJ'),\n",
       " ('rubble', 'JJ'),\n",
       " ('sing', 'VBG'),\n",
       " ('rebel', 'NN'),\n",
       " ('song', 'NN'),\n",
       " ('Do', 'NNP'),\n",
       " ('want', 'VB'),\n",
       " ('see', 'VB'),\n",
       " ('another', 'DT'),\n",
       " ('generation', 'NN'),\n",
       " ('drop', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('rather', 'RB'),\n",
       " ('comma', 'VBP'),\n",
       " ('full', 'JJ'),\n",
       " ('stop', 'NN'),\n",
       " ('Maybe', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('black', 'JJ'),\n",
       " ('maybe', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('knee', 'VBP'),\n",
       " ('Maybe', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('gap', 'VBP'),\n",
       " ('two', 'CD'),\n",
       " ('trapeze', 'NN'),\n",
       " ('But', 'CC'),\n",
       " ('heart', 'NN'),\n",
       " ('beating', 'NN'),\n",
       " ('pulse', 'JJ'),\n",
       " ('start', 'NN'),\n",
       " ('Cathedrals', 'NNP'),\n",
       " ('heart', 'NN'),\n",
       " ('As', 'IN'),\n",
       " ('saw', 'JJ'),\n",
       " ('oh', 'IN'),\n",
       " ('light', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('swear', 'VBP'),\n",
       " ('emerge', 'RB'),\n",
       " ('blinking', 'VBG'),\n",
       " ('To', 'TO'),\n",
       " ('tell', 'VB'),\n",
       " ('alright', 'RB'),\n",
       " ('As', 'IN'),\n",
       " ('soar', 'NN'),\n",
       " ('wall', 'NN'),\n",
       " ('every', 'DT'),\n",
       " ('siren', 'NN'),\n",
       " ('symphony', 'NN'),\n",
       " ('And', 'CC'),\n",
       " ('every', 'DT'),\n",
       " ('tear', 'NN'),\n",
       " ('waterfall', 'NN'),\n",
       " ('Is', 'VBZ'),\n",
       " ('waterfall', 'JJ'),\n",
       " ('Oh', 'NNP'),\n",
       " ('Is', 'NNP'),\n",
       " ('waterfall', 'JJ'),\n",
       " ('Oh', 'NNP'),\n",
       " ('oh', 'MD'),\n",
       " ('oh', 'VB'),\n",
       " ('Is', 'NNP'),\n",
       " ('waterfall', 'JJ'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Is', 'VBZ'),\n",
       " ('waterfall', 'JJ'),\n",
       " ('Oh', 'NNP'),\n",
       " ('oh', 'MD'),\n",
       " ('oh', 'VB'),\n",
       " ('So', 'NNP'),\n",
       " ('hurt', 'JJ'),\n",
       " ('hurt', 'NN'),\n",
       " ('bad', 'JJ'),\n",
       " ('But', 'CC'),\n",
       " ('still', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('raise', 'VBP'),\n",
       " ('flag', 'JJ'),\n",
       " ('Oh', 'IN'),\n",
       " ('It', 'PRP'),\n",
       " ('wa', 'VBZ'),\n",
       " ('wa', 'JJ'),\n",
       " ('wa', 'NN'),\n",
       " ('wa', 'VBD'),\n",
       " ('A', 'NNP'),\n",
       " ('wa', 'NN'),\n",
       " ('wa', 'NN'),\n",
       " ('wa', 'NN'),\n",
       " ('wa', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('teardrop', 'NN'),\n",
       " ('waterfall', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('teardrop', 'NN'),\n",
       " ('waterfall', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('tear', 'NN'),\n",
       " ('Every', 'NNP'),\n",
       " ('teardrop', 'NN'),\n",
       " ('waterfall', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use the function pos_tag of NLTK to perform POS-tagging and print the result\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag(lem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca7e10",
   "metadata": {},
   "source": [
    "As you can see, it does not return values like 'a', 'n', 'v' or 'r' as the WordNet lemmatizer is expecting...\n",
    "\n",
    "So we have to convert the values from the NLTK POS-tagging to put them into the WordNet Lemmatizer. This is done in the function get_wordnet_pos written below. Try to understand it, and then we will reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02cdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c53c5",
   "metadata": {},
   "source": [
    "So now you have all we need to perform properly the lemmatization.\n",
    "\n",
    "So you have to use the following to do so:\n",
    "* your tags from the POS-tagging performed\n",
    "* the function *get_wordnet_pos*\n",
    "* the *WordNetLemmatizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cef0054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'music', 'I', 'get', 'record', 'I', 'shut', 'world', 'outside', 'light', 'come', 'Maybe', 'street', 'alight', 'maybe', 'tree', 'go', 'I', 'feel', 'heart', 'start', 'beat', 'favourite', 'song', 'And', 'kid', 'dance', 'kid', 'night', 'Until', 'Monday', 'morning', 'feel', 'another', 'life', 'I', 'turn', 'music', 'I', 'roll', 'time', 'And', 'heaven', 'sight', 'I', 'turn', 'music', 'I', 'get', 'record', 'From', 'underneath', 'rubble', 'sing', 'rebel', 'song', 'Do', 'want', 'see', 'another', 'generation', 'drop', 'I', 'rather', 'comma', 'full', 'stop', 'Maybe', 'I', 'black', 'maybe', 'I', 'knee', 'Maybe', 'I', 'gap', 'two', 'trapeze', 'But', 'heart', 'beating', 'pulse', 'start', 'Cathedrals', 'heart', 'As', 'saw', 'oh', 'light', 'I', 'swear', 'emerge', 'blink', 'To', 'tell', 'alright', 'As', 'soar', 'wall', 'every', 'siren', 'symphony', 'And', 'every', 'tear', 'waterfall', 'Is', 'waterfall', 'Oh', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'waterfall', 'Every', 'tear', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'hurt', 'hurt', 'bad', 'But', 'still', 'I', 'raise', 'flag', 'Oh', 'It', 'wa', 'wa', 'wa', 'wa', 'A', 'wa', 'wa', 'wa', 'wa', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform the lemmatization properly\n",
    "lem_words2 = [lemmatizer.lemmatize(w[0], w[1]) for w in get_wordnet_pos(pos_tag(lem_words))]\n",
    "print(lem_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abdbff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'turn', 'music', 'i', 'got', 'record', 'i', 'shut', 'world', 'outsid', 'light', 'come', 'mayb', 'street', 'alight', 'mayb', 'tree', 'gone', 'i', 'feel', 'heart', 'start', 'beat', 'favourit', 'song', 'and', 'kid', 'danc', 'kid', 'night', 'until', 'monday', 'morn', 'feel', 'anoth', 'life', 'i', 'turn', 'music', 'i', 'roll', 'time', 'and', 'heaven', 'sight', 'i', 'turn', 'music', 'i', 'got', 'record', 'from', 'underneath', 'rubbl', 'sing', 'rebel', 'song', 'do', 'want', 'see', 'anoth', 'gener', 'drop', 'i', 'rather', 'comma', 'full', 'stop', 'mayb', 'i', 'black', 'mayb', 'i', 'knee', 'mayb', 'i', 'gap', 'two', 'trapez', 'but', 'heart', 'beat', 'puls', 'start', 'cathedr', 'heart', 'as', 'saw', 'oh', 'light', 'i', 'swear', 'emerg', 'blink', 'to', 'tell', 'alright', 'as', 'soar', 'wall', 'everi', 'siren', 'symphoni', 'and', 'everi', 'tear', 'waterfal', 'is', 'waterfal', 'oh', 'is', 'waterfal', 'oh', 'oh', 'oh', 'is', 'waterfal', 'everi', 'tear', 'is', 'waterfal', 'oh', 'oh', 'oh', 'so', 'hurt', 'hurt', 'bad', 'but', 'still', 'i', 'rais', 'flag', 'oh', 'it', 'wa', 'wa', 'wa', 'wa', 'a', 'wa', 'wa', 'wa', 'wa', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform stemming\n",
    "stemmed_words = [stemmer.stem(w) for w in lyrics_no_stops]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60c26a",
   "metadata": {},
   "source": [
    "Do you see the difference? What would you use?\n",
    "\n",
    "stemming incorrectly converted some words (eg. 'Maybe' -> 'mayb', 'morning' -> 'morn', 'Every' -> 'everi').\n",
    "Using lemmatization with POS-tagging will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5785eb",
   "metadata": {},
   "source": [
    "## 1.2. Bag-of-words\n",
    "Now that we master the preprocessing, let's make our first Bag Of Words (BOW).</p></p>\n",
    "We will reuse our dataset of Coldplay songs to make a BOW.</p>\n",
    "Open the BOW.ipynb notebook and follow the instructions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45cc46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6b559",
   "metadata": {},
   "source": [
    "Load now the dataset in coldplay.csv using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d487eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Link</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Another's Arms</td>\n",
       "      <td>/c/coldplay/anothers+arms_21079526.html</td>\n",
       "      <td>Late night watching tv  \\nUsed to be you here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Bigger Stronger</td>\n",
       "      <td>/c/coldplay/bigger+stronger_20032648.html</td>\n",
       "      <td>I want to be bigger stronger drive a faster ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>/c/coldplay/daylight_20032625.html</td>\n",
       "      <td>To my surprise, and my delight  \\nI saw sunris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Everglow</td>\n",
       "      <td>/c/coldplay/everglow_21104546.html</td>\n",
       "      <td>Oh, they say people come  \\nThey say people go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Every Teardrop Is A Waterfall</td>\n",
       "      <td>/c/coldplay/every+teardrop+is+a+waterfall_2091...</td>\n",
       "      <td>I turn the music up, I got my records on  \\nI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Artist                           Song  \\\n",
       "0  Coldplay                 Another's Arms   \n",
       "1  Coldplay                Bigger Stronger   \n",
       "2  Coldplay                       Daylight   \n",
       "3  Coldplay                       Everglow   \n",
       "4  Coldplay  Every Teardrop Is A Waterfall   \n",
       "\n",
       "                                                Link  \\\n",
       "0            /c/coldplay/anothers+arms_21079526.html   \n",
       "1          /c/coldplay/bigger+stronger_20032648.html   \n",
       "2                 /c/coldplay/daylight_20032625.html   \n",
       "3                 /c/coldplay/everglow_21104546.html   \n",
       "4  /c/coldplay/every+teardrop+is+a+waterfall_2091...   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Late night watching tv  \\nUsed to be you here ...  \n",
       "1  I want to be bigger stronger drive a faster ca...  \n",
       "2  To my surprise, and my delight  \\nI saw sunris...  \n",
       "3  Oh, they say people come  \\nThey say people go...  \n",
       "4  I turn the music up, I got my records on  \\nI ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset in coldplay.csv\n",
    "coldplay = pd.read_csv('coldplay.csv')\n",
    "coldplay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2db74b",
   "metadata": {},
   "source": [
    "Now using the CountVectorizer of scikit-learn, make a BOW of all the lyrics of Coldplay, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02174e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1569)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute a BOW\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "BOW = vectorizer.fit_transform(coldplay['Lyrics'])\n",
    "print(BOW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeebbb",
   "metadata": {},
   "source": [
    "Now that we have the BOW matrix, we would like to have a new dataframe having the BOW for each song, and as columns the corresponding words (just as we did in the lecture at the end).\n",
    "\n",
    "So that at the end we would end up with a dataframe containing something like the following (120 raws for 120 songs, and as many columns as words):\n",
    "\n",
    "| | ah | adventure | ... | yeah \n",
    "|---|---|---|---|---| \n",
    "| 0 | 0 | 1 | ... | 4 |\n",
    "| 1 | 8 | 0 | ... | 2 |\n",
    "|...|...|...|...|...|\n",
    "| 119 | 5 | 0 | ... | 8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817821c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2000</th>\n",
       "      <th>2gether</th>\n",
       "      <th>76543</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>achin</th>\n",
       "      <th>adventure</th>\n",
       "      <th>advice</th>\n",
       "      <th>...</th>\n",
       "      <th>x2</th>\n",
       "      <th>x7</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "      <th>yuletide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  2000  2gether  76543  aaaaaah  aaaaah  aaaah  achin  adventure  \\\n",
       "0     0     0        0      0        0       0      0      0          0   \n",
       "1     0     0        0      0        0       0      0      0          0   \n",
       "2     0     0        0      0        0       0      0      0          0   \n",
       "3     0     0        0      0        0       0      0      0          0   \n",
       "4     0     0        0      0        0       0      0      0          0   \n",
       "..   ..   ...      ...    ...      ...     ...    ...    ...        ...   \n",
       "115   0     0        0      0        0       0      0      0          0   \n",
       "116   0     0        0      0        0       0      0      0          0   \n",
       "117   0     0        1      0        0       0      0      0          0   \n",
       "118   0     0        0      0        0       0      0      0          0   \n",
       "119   0     0        0      0        0       0      0      0          0   \n",
       "\n",
       "     advice  ...  x2  x7  ya  yeah  years  yellow  yes  yesterday  young  \\\n",
       "0         0  ...   0   0   0     0      0       0    0          0      0   \n",
       "1         0  ...   0   0   0     0      0       0    0          0      0   \n",
       "2         0  ...   0   0   0     2      0       0    0          0      0   \n",
       "3         0  ...   0   0   0     2      0       0    0          0      0   \n",
       "4         0  ...   0   0   0     0      0       0    0          0      0   \n",
       "..      ...  ...  ..  ..  ..   ...    ...     ...  ...        ...    ...   \n",
       "115       0  ...   0   0   0     0      0       0    0          0      0   \n",
       "116       0  ...   0   0   0    11      0       0    0          0      0   \n",
       "117       0  ...   0   0   0     3      0       0    0          0      0   \n",
       "118       0  ...   0   0   0     0      0       0    0          0      0   \n",
       "119       0  ...   0   0   0     0      0       0    0          0      0   \n",
       "\n",
       "     yuletide  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "115         0  \n",
       "116         0  \n",
       "117         0  \n",
       "118         0  \n",
       "119         0  \n",
       "\n",
       "[120 rows x 1569 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a new dataframe containing the BOW outputs and the corresponding words as columns. And print it\n",
    "# Get the words associated to those numbers\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "bow_df = pd.DataFrame(data=BOW.toarray(), columns=tokens)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d26003",
   "metadata": {},
   "source": [
    "Well as you see we're still having some issue, we have some tokens that are not words, like '10' or '2000'.\n",
    "\n",
    "To get rid of that, we could use directly regular expressions within the function. Another solution would be to make preprocessing before using the function *CountVectorizer*.\n",
    "\n",
    "For the moment, we won't pay attention to this issue. But if you are curious and have time, you can find on google how to remove those words using the *CountVectorizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f906c",
   "metadata": {},
   "source": [
    "Now we would like to see what are the most used words by Coldplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e7b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_bow = bow_df.sum()\n",
    "sum_bow.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af51861",
   "metadata": {},
   "source": [
    "Now make a sort in order to show the 10 most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a4e7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oh      334\n",
       "don     190\n",
       "know    137\n",
       "just    136\n",
       "ll      132\n",
       "come    126\n",
       "yeah    111\n",
       "love     95\n",
       "ooh      95\n",
       "want     86\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: print the 10 most used word by Coldplay\n",
    "sum_bow.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecc049",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ed943",
   "metadata": {},
   "source": [
    "# 2. Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a2d0d",
   "metadata": {},
   "source": [
    "## 2.1. Similarity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138daf0b",
   "metadata": {},
   "source": [
    "We will work on applying similarity: Jaccard and Cosine similarity. This exercise is a simple\n",
    "application.\n",
    "\n",
    "Open Similarity.ipynb notebook and follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a23c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9952c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"Outside the classroom, Stallman pursued his studies with even more diligence, rushing off to fulfill his laboratory-assistant duties at Rockefeller University during the week and dodging the Vietnam protesters on his way to Saturday school at Columbia. It was there, while the rest of the Science Honors Program students sat around discussing their college choices, that Stallman finally took a moment to participate in the preclass bull session.\"\n",
    "B = \"To facilitate the process, AI Lab hackers had built a system that displayed both the source and display modes on a split screen. Despite this innovative hack, switching from mode to mode was still a nuisance.\"\n",
    "C = \"With no dorm and no dancing, Stallman's social universe imploded. Like an astronaut experiencing the aftereffects of zero-gravity, Stallman found that his ability to interact with nonhackers, especially female nonhackers, had atrophied significantly. After 16 weeks in the AI Lab, the self confidence he'd been quietly accumulating during his 4 years at Harvard was virtually gone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf1025",
   "metadata": {},
   "source": [
    "Begin by computing the Jaccard Similarity J of all possibilities:\n",
    "* J(A, B)\n",
    "* J(B, C)\n",
    "* J(A, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "769d5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_similarity AB: 0.08536585365853659\n",
      "Jaccard_similarity BC: 0.09210526315789473\n",
      "Jaccard_similarity AC: 0.125\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute the Jaccard similarities\n",
    "# Split the sentences\n",
    "\n",
    "# Compute the intersection and union\n",
    "\n",
    "# Compute and print the Jaccard Similarity\n",
    "\n",
    "def intersection(doc_1, doc_2):\n",
    "    return doc_1.intersection(doc_2)\n",
    "\n",
    "def union(doc_1, doc_2):\n",
    "    return doc_1.union(doc_2)\n",
    "\n",
    "def jaccard_similarity(doc_1, doc_2):\n",
    "    \n",
    "    tokens_doc_1 = set([t for t in doc_1.split(' ')])\n",
    "    tokens_doc_2 = set([t for t in doc_2.split(' ')])\n",
    "\n",
    "    intersection_docs = intersection(tokens_doc_1, tokens_doc_2)\n",
    "\n",
    "    union_docs = union(tokens_doc_1, tokens_doc_2)\n",
    "    \n",
    "    return len(intersection_docs) / len(union_docs)\n",
    "\n",
    "print(\"Jaccard_similarity AB: %s\" % (jaccard_similarity(A, B)))\n",
    "print(\"Jaccard_similarity BC: %s\" % (jaccard_similarity(B, C)))\n",
    "print(\"Jaccard_similarity AC: %s\" % (jaccard_similarity(C, A)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e47dc",
   "metadata": {},
   "source": [
    "What are the closest to the other according to Jaccard Similarity?\n",
    "\n",
    "CA, has the highest Jaccard Similarity (12.5%).\n",
    "\n",
    "Now let's do the same using TF-IDF and Cosine Similarity. Compute the TF-IDF and cosine similarities and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dba24575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(A, B): 0.16793269576264072\n",
      "cos(B, C): 0.13618963113796592\n",
      "cos(A, C): 0.2850296032333907\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute the TF-IDF of A, B and C and the cosine similarities of all possibilities\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "documents = [A, B, C]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print('cos(A, B):', cosine_similarities[0][1])\n",
    "print('cos(B, C):', cosine_similarities[1][2])\n",
    "print('cos(A, C):', cosine_similarities[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ca3e3",
   "metadata": {},
   "source": [
    "## 2.2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dead2fd",
   "metadata": {},
   "source": [
    "We will compute the TF-IDF on a corpus of newspaper headlines.\n",
    "\n",
    "Open TF-IDF.ipynb notebook and follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3bceb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87539b",
   "metadata": {},
   "source": [
    "Import the data into the file headlines.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "790264b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "headlines = pd.read_csv('headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7f0b3",
   "metadata": {},
   "source": [
    "check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "289f27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20170721  algorithms can make decisions on behalf of fed...\n",
      "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2      20170721                           a rural mural in thallan\n",
      "3      20170721  australia church risks becoming haven for abusers\n",
      "4      20170721  australian company usgfx embroiled in shanghai...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "print(headlines.iloc[0:5])\n",
    "headlines.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0957ee6",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, pandas's method apply, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d90a29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [algorithm, make, decis, behalf, feder, minist]\n",
       "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
       "2                                 [rural, mural, thallan]\n",
       "3                  [australia, church, risk, becom, abus]\n",
       "4       [australian, compani, usgfx, embroil, shanghai...\n",
       "                              ...                        \n",
       "1994    [constitut, avenu, win, top, prize, act, archi...\n",
       "1995                         [dark, mofo, number, crunch]\n",
       "1996    [david, petraeu, say, australia, must, firm, s...\n",
       "1997    [driverless, car, australia, face, challeng, r...\n",
       "1998               [drug, compani, criticis, price, hike]\n",
       "Name: headline_stem, Length: 1999, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform preprocessing\n",
    "# import needed modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return [t for t in text if t.isalpha()]\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return [t for t in text if t not in stop_words]\n",
    "\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in text]\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "headlines['headline_Tokenized'] = headlines['headline_text'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation\n",
    "headlines['headline_remove_punctuation'] = headlines['headline_Tokenized'].apply(remove_punctuation)\n",
    "\n",
    "# Remove stop words\n",
    "headlines['headline_no_stop_words'] = headlines['headline_remove_punctuation'].apply(remove_stop_words)\n",
    "\n",
    "# Stem\n",
    "headlines['headline_stem'] = headlines['headline_no_stop_words'].apply(stem)\n",
    "\n",
    "\n",
    "headlines['headline_stem']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb58e6",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bc57fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 4165)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False, analyzer=lambda x: x)\n",
    "BOW = vectorizer.fit_transform(headlines['headline_stem']).toarray()\n",
    "\n",
    "print(BOW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618339d",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`.\n",
    "\n",
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b3a6df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
       "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
       "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
       "       0.5       , 1.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "TF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names_out())\n",
    "TF = TF.divide(TF.sum(axis=1), axis=0)\n",
    "np.unique(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d296c121",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.28291422, 3.36629583, 3.44151925, 3.53995932, 3.57505064,\n",
       "       3.70858204, 3.79373984, 3.83920222, 3.91152288, 3.96281617,\n",
       "       4.04505427, 4.10389477, 4.13466643, 4.16641513, 4.19920495,\n",
       "       4.2331065 , 4.26819782, 4.30456547, 4.3423058 , 4.38152651,\n",
       "       4.4223485 , 4.46490812, 4.50935988, 4.5558799 , 4.60467006,\n",
       "       4.65596336, 4.71003058, 4.76718899, 4.82781361, 4.89235213,\n",
       "       4.961345  , 5.03545298, 5.11549568, 5.20250706, 5.29781724,\n",
       "       5.40317776, 5.52096079, 5.65449219, 5.80864287, 5.99096442,\n",
       "       6.21410797, 6.50179005, 6.90725515, 7.60040233])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "IDF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names_out())\n",
    "IDF[IDF>1] = 1\n",
    "IDF = np.log(len(IDF)/IDF.sum(axis=0))\n",
    "np.unique(IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec0868",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e93f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "tfidf = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d4839",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32703f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words:  gcfc    0.633367\n",
      "geel    0.633367\n",
      "gw      0.633367\n",
      "haw     0.633367\n",
      "melb    0.633367\n",
      "coll    0.633367\n",
      "adel    0.633367\n",
      "syd     0.633367\n",
      "nmfc    0.633367\n",
      "cold    0.690456\n",
      "dtype: float64\n",
      "highest words:  date         3.800201\n",
      "mongolian    3.800201\n",
      "puffbal      3.800201\n",
      "mous         3.800201\n",
      "rig          3.800201\n",
      "superannu    3.800201\n",
      "aquapon      3.800201\n",
      "loophol      3.800201\n",
      "pump         6.907255\n",
      "peacemak     7.600402\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "print('lowest words: ', tfidf.max(axis=0).sort_values()[:10])\n",
    "print('highest words: ', tfidf.max(axis=0).sort_values()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95ce97",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2053142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "# Import the module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "# Compute the TF-IDF\n",
    "tf_idf = vectorizer.fit_transform(headlines['headline_stem']).toarray()\n",
    "tf_idf = pd.DataFrame(data=tf_idf, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6f64f",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3698cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words:  coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "highest words:  mosul        0.779137\n",
      "rig          0.786813\n",
      "travel       0.788050\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "employ       0.795060\n",
      "financ       0.803629\n",
      "mongolian    0.831769\n",
      "pump         1.000000\n",
      "peacemak     1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "print('lowest words: ', tf_idf.max(axis=0).sort_values()[:10])\n",
    "print('highest words: ', tf_idf.max(axis=0).sort_values()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3370b4",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01011c3",
   "metadata": {},
   "source": [
    "the highest TF-IDF on average is peacemak (7.600402)\n",
    "the lowest TF-IDF on average is gcfc, geel, gw, haw, melb, coll, adel\n",
    ", syd, nmfc (0.633367)\n",
    "\n",
    "\n",
    "And for TF-IDF using scikit-learn,\n",
    "the highest TF-IDF on average is pump and peacemak (1.000000)\n",
    "the lowest TF-IDF on average is coll, gw, nmfc, adel, melb, syd, haw, geel and gcfc (0.305258)\n",
    "\n",
    "The result of scikit-learn TF-IDF is slightly different from the manual TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75ca4d",
   "metadata": {},
   "source": [
    "## 2.3. Plagiarism checker (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7e742",
   "metadata": {},
   "source": [
    "In the folder, you will find source texts (Asource.txt, Bsource.txt, Csource.txt and Dsource.txt) from which some texts were inspired (A1.txt was inspired from Asource.txt and so on). Some are\n",
    "plagiarism, some are regular inspiration.\n",
    "\n",
    "Your job is to use text similarity to define a plagiarism detection algorithm based on those short\n",
    "examples.\n",
    "\n",
    "In the examples, there are sources, direct plagiarism (A1, B1, C1, D1) and sometimes examples of so\n",
    "called mosaic plagiarism (D2). Can you make an algorithm that detects all kind of plagiarism without\n",
    "false positive?\n",
    "\n",
    "Open Plagiarism.ipynb notebook and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd62f8f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc461476",
   "metadata": {},
   "source": [
    "# 3. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b1710",
   "metadata": {},
   "source": [
    "It's time to make our first real Machine Learning application of NLP: a spam classifier!\n",
    "A spam classifier is a Machine Learning model that classifier texts (email or SMS) into two\n",
    "categories: Spam (1) or legitimate (0).\n",
    "\n",
    "To do that, we will reuse our knowledge: we will apply preprocessing and BOW (Bag Of Words) on\n",
    "a dataset of texts.\n",
    "\n",
    "Then we will use a classifier to predict to which class belong a new email/SMS, based on the BOW.\n",
    "Open Spam-Classifier.ipynb notebook and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296a239",
   "metadata": {},
   "source": [
    "First things first: import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0f4fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bdb73",
   "metadata": {},
   "source": [
    "Load now the dataset in spam.csv using pandas. Use the 'latin-1' encoding as loading option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "005c2d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                            Message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset \n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65007966",
   "metadata": {},
   "source": [
    "As usual, I suggest you to explore a bit this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c02ec115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Class    5572 non-null   object\n",
      " 1   Message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: explore the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f7863",
   "metadata": {},
   "source": [
    "So as you see we have a column containing the labels, and a column containing the text to classify.\n",
    "\n",
    "We will begin by doing the usual preprocessing: tokenization, punctuation removal and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7641ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_65068\\2288655835.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Message'] = df['Message'].str.replace('[^a-zA-Z0-9 ]','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Go, until, jurong, point, crazy, Available, o...\n",
      "1                          [Ok, lar, Joking, wif, u, oni]\n",
      "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3       [U, dun, say, so, early, hor, U, c, already, t...\n",
      "4       [Nah, I, dont, think, he, go, to, usf, he, lif...\n",
      "                              ...                        \n",
      "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
      "5568            [Will, b, going, to, esplanade, fr, home]\n",
      "5569    [Pity, wa, in, mood, for, that, Soany, other, ...\n",
      "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
      "5571                      [Rofl, Its, true, to, it, name]\n",
      "Name: tokens, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform preprocessing over all the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "df['Message'] = df['Message'].str.replace('[^a-zA-Z0-9 ]','')\n",
    "df['tokens'] = df['Message'].apply(word_tokenize)\n",
    "df['tokens'] = df['tokens'].apply (lemmatize_text)\n",
    "\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e1001",
   "metadata": {},
   "source": [
    "Ok now we have our preprocessed data. Next step is to do a BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33b6553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "BOW = vectorizer.fit_transform(df['Message']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79dbf6a",
   "metadata": {},
   "source": [
    "Then make a new dataframe as usual to have a visual idea of the words used and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f52248cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   008704050406  0089my  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0             0       0     0            0            0           0   0   \n",
       "1             0       0     0            0            0           0   0   \n",
       "2             0       0     0            0            0           0   0   \n",
       "3             0       0     0            0            0           0   0   \n",
       "4             0       0     0            0            0           0   0   \n",
       "\n",
       "   020603  0207  02070836089  ...  zebra  zed  zeros  zhong  zindgi  zoe  \\\n",
       "0       0     0            0  ...      0    0      0      0       0    0   \n",
       "1       0     0            0  ...      0    0      0      0       0    0   \n",
       "2       0     0            0  ...      0    0      0      0       0    0   \n",
       "3       0     0            0  ...      0    0      0      0       0    0   \n",
       "4       0     0            0  ...      0    0      0      0       0    0   \n",
       "\n",
       "   zogtorius  zoom  zouk  zyada  \n",
       "0          0     0     0      0  \n",
       "1          0     0     0      0  \n",
       "2          0     0     0      0  \n",
       "3          0     0     0      0  \n",
       "4          0     0     0      0  \n",
       "\n",
       "[5 rows x 9271 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a new dataframe with the BOW\n",
    "bow_df = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names_out())\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc147b",
   "metadata": {},
   "source": [
    "Let's check what is the most used word in the spam category and the non spam category.\n",
    "\n",
    "There are two steps: first add the class to the BOW dataframe. Second, filter on a class, sum all the values and print the most frequent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0526a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequent spam word: free\n",
      "most frequent non spam word: im\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the most used word in the spam and non spam category\n",
    "tmp = bow_df\n",
    "tmp['Class'] = df['Class']\n",
    "most_frequent_spam = tmp[tmp['Class'] == 'spam'].drop(columns='Class').sum().idxmax()\n",
    "most_frequent_non_spam = tmp[tmp['Class'] == 'ham'].drop(columns='Class').sum().idxmax()\n",
    "\n",
    "print('most frequent spam word:', most_frequent_spam)\n",
    "print('most frequent non spam word:', most_frequent_non_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31edff",
   "metadata": {},
   "source": [
    "You should find that the most frequent spam word is 'free', not so surprising, right?\n",
    "\n",
    "Now we can make a classifier based on our BOW. We will use a simple logistic regression here for the example.\n",
    "\n",
    "You're an expert, you know what to do, right? Split the data, train your model, predict and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b498deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9748878923766816"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform a classification to predict whether a message is a spam or not\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['Class'])\n",
    "print(le.transform(df['Class']))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bow_df.drop(columns='Class'),\n",
    "    le.transform(df['Class']),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "test_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cb621e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b69ead",
   "metadata": {},
   "source": [
    "What precision do you get? Check by hand on some samples where it did predict well to check what could go wrong...\n",
    "\n",
    "Try to use other models and try to improve your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7f96e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "matrix = confusion_matrix(y_test, test_pred)\n",
    "sns.heatmap(matrix, annot=True, fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2e95d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e0f49",
   "metadata": {},
   "source": [
    "# 4. Word Embedding (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba93970",
   "metadata": {},
   "source": [
    "Open quora.ipynb and follow the instructions.\n",
    "\n",
    "__Objectives__\n",
    "\n",
    "Quora is a popular website where anyone can ask and/or answer a question. There are more than 100\n",
    "millions unique visitors per month.\n",
    "\n",
    "Like any other forum, Quora is facing a problem: toxic questions and comments.\n",
    "\n",
    "As you can imagine, Quora teams cannot check all of the Q&A by hand. So they decided to ask the\n",
    "data science community to help them to perform automatically insincere questions classification.\n",
    "\n",
    "__Guidelines__\n",
    "\n",
    "This challenge was launched on Kaggle : https://www.kaggle.com/c/quora-insincere-questionsclassification\n",
    "\n",
    "Read the overall information on Kaggle. Quora provided a dataset of questions with a label, and the\n",
    "features are the following:\n",
    "\n",
    "* qid: a unique identifier for each question, a hexadecimal number\n",
    "* question_text: the text of the question\n",
    "* target: either 1 (for insincere question) or 0\n",
    "\n",
    "The Kaggle dataset is quite heavy and it may be too difficult for your laptops to perform the\n",
    "computations. Therefore, we provide you with __the train dataset__ (to be sampled) and also __light word embeddings__, which you can download here\n",
    "\n",
    "Don't look at the published kernels, in order to keep your judgement unbiased.\n",
    "\n",
    "Here are a few steps to follow:\n",
    "1. First sample the dataset to 10.000 lines otherwise your laptop might die (you may need to use sklearn.utils.resample()).\n",
    "2. As usual, begin with a proper EDA.\n",
    "3. Perfom a nice text preprocessing.\n",
    "4. Try to run a quick sentiment analysis using TextBlob.\n",
    "5. Then, use a word embedding (Glove) to create your corpus and run your model.\n",
    "6. Do some optimization (text preprocessing, model hyperparameters, other word embeddings if you trust your computer).\n",
    "7. Optimize ++: Now, let's have a look at some published kernels and find some inspiration.\n",
    "8. Bonus question : try to identify the most recurrent topics in toxic questions !\n",
    "In this competition, the metric used for performance evaluation is the __F-score__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727143bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
